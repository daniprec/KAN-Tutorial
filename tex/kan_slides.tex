\documentclass[aspectratio=169]{beamer}
\usetheme{ie} % <— this loads beamerthemeie.sty

% If you use minted, compile with: -shell-escape
\usepackage{amsmath, amssymb, mathtools, bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{hyperref}
% \usepackage{minted}
% \setminted{fontsize=\footnotesize, breaklines, autogobble, frame=lines}

\usepackage[backend=biber,style=authoryear,maxbibnames=3]{biblatex}
\addbibresource{references.bib}

% Optional: set short versions for footer
\title[Kolmogorov–Arnold Networks]{KAN Tutorial Slides}
\author[Daniel Precioso]{Daniel Precioso Garcelán}
\date{\today}

\begin{document}
\maketitle

\begin{frame}{Kolmogorov-Arnold Representation Theorem}
Let $ \Omega \subset \mathbb{R}^d $ be a bounded domain and let $ f: \Omega \rightarrow \mathbb{R} $ be a continuous function; i.e. $ f \in C(\Omega) $.

Then there exist continuous univariate functions
$$
\Phi_q: \mathbb{R} \to \mathbb{R}, \quad q = 1, \dots, 2d+1;
$$
and continuous univariate functions
$$
\phi_{pq}: \mathbb{R} \to \mathbb{R}, \quad p = 1, \dots, d; \quad q = 1, \dots, 2d+1;
$$
such that for every $\mathbf{x} = (x_1, \dots, x_d) \in \Omega$,
$$
f(\mathbf{x}) = \sum_{q=1}^{2d+1} \Phi_q \left( \sum_{p=1}^d \phi_{pq}(x_p) \right).
$$
\end{frame}

%------------------------------------------

\begin{frame}{Kolmogorov–Arnold Representation Theorem}
	
	\begin{columns}[T,onlytextwidth]
		
		% --- Left column: diagram ---
		\column{0.55\textwidth}
		\centering
		\resizebox{!}{0.8\textheight}{\input{an_251.tex}}
		
		% --- Right column: explanation ---
		\column{0.4\textwidth}
		The theorem states that any $f(x_1, x_2)$ can be written as a sum of univariate compositions.
		
		\vspace{0.8em}
		The diagram shows this expression visually: each block represents a component of the decomposition.
		
		\vspace{0.8em}
		Together, they form a \textbf{Kolmogorov–Arnold Network (KAN)}.
		
	\end{columns}
	
\end{frame}

%------------------------------------------

\begin{frame}{Kolmogorov–Arnold Networks}
	
	\begin{columns}[T,onlytextwidth]
		
		% --- Left column: diagram ---
		\column{0.55\textwidth}
		\centering
		\resizebox{!}{0.8\textheight}{\input{kan_251.tex}}
		
		% --- Right column: explanation ---
		\column{0.4\textwidth}
		In a network setting, each univariate function is written as $\phi_{d,p,q}$, where:
		\vspace{0.8em}
		\begin{itemize}
			\item $d$: layer depth  
			\item $p$: output node index  
			\item $q$: input node index
		\end{itemize}
		
		\vspace{1em}
		This network is a \textbf{KAN [2,5,1]}:\\it has 2 inputs, one hidden layer with 5 nodes, and 1 output.
		
	\end{columns}
	
\end{frame}

%------------------------------------------

\begin{frame}{Kolmogorov–Arnold Networks}
	\begin{columns}[T,onlytextwidth]
		
		% --- Left column: diagram ---
		\column{0.5\textwidth}
		\centering
		\resizebox{!}{0.8\textheight}{\input{kan_2331.tex}}
		
		% --- Right column: explanation ---
		\column{0.45\textwidth}
		\textbf{KAN [2,3,3,1]}: 2 inputs, two hidden layers of 3, 1 output.
		
		\vspace{0.8em}
		\textit{Why go deeper?}
		\begin{itemize}
			\item \textbf{Theory:} Any continuous $f(\mathbf{x})$ admits a shallow KAN $[n_{\text{in}},\,2\ n_{\text{in}} + 1,\,1]$.
			\item \textbf{Practice:} Deeper KANs can model non-continuous functions. Depth improves expressivity.
		\end{itemize}
		
	\end{columns}
\end{frame}

%------------------------------------------

\begin{frame}{B-Splines}
	$\phi_{d,p,q}$ can be chosen from any family of continuous univariate functions. A common choice is the \textbf{B-spline} family.
	
	A B-spline of degree $k$ is defined as:
	
	$$B_k(x) = \sum_{i=1}^{n} P_i N_{i,k}(x)$$
	
	where $n$ is the number of control points,\\
	$N_{i,k}$ are the basis functions of degree $k$,\\
	and $P_i$ are the control points (spline weights).
\end{frame}

%------------------------------------------

\begin{frame}{B-Splines}
The basis functions follow the standard \textbf{Cox–de Boor recursive definition}:

\begin{flalign*}
	N_{i,0}(x) &= 
	\begin{cases}
		1, & t_i \le x < t_{i+1} \\
		0, & \text{otherwise}
	\end{cases} &
\end{flalign*}

\begin{flalign*}
	N_{i,k}(x) &=
	\frac{x - t_i}{t_{i+k} - t_i} N_{i,k-1}(x)
	+
	\frac{t_{i+k+1} - x}{t_{i+k+1} - t_{i+1}} N_{i+1,k-1}(x),
	\quad k > 0 &
\end{flalign*}

where ${t_i} \in [t_1, t_m]$ is the \textbf{knot vector}, a non-decreasing sequence of real numbers of length $m = n + k + 1$.
\end{frame}

%------------------------------------------

\begin{frame}{B-Splines as KAN Edges}
	
	All univariate functions share the same spline degree $k$ and number of control points $n$.  
	Each $\phi_{d,p,q}$ combines a basis function (similar to residual connections) with a B-spline expansion:
	
	\begin{flalign*}
		\phi(x) = w_b\, b(x) + \sum_{i=1}^{n} P_i\, N_{i,k}(x)
	\end{flalign*}
	
	Here, $w_b$ is the learnable weight of the basis function, and the control point coefficients $P_i$ scale the individual B-spline functions directly.
	
	We choose the basis as:
	
	\begin{flalign*}
		b(x) = \mathrm{SiLU}(x) = \frac{x}{1 + e^{-x}}
	\end{flalign*}
	
\end{frame}

%------------------------------------------

\begin{frame}{KAN Parameters}
	
	\begin{columns}[T,onlytextwidth]
		
		% --- Left column: diagram ---
		\column{0.5\textwidth}
		\centering
		\resizebox{!}{0.8\textheight}{\input{kan_251.tex}}
		
		% --- Right column: explanation ---
		\column{0.45\textwidth}
		
		\textbf{Hyperparameters}
		\begin{itemize}
			\item $n$: number of control points.
			\item $k$: B-spline degree.
			\item $m$: number of knots, $m = n + k + 1$
		\end{itemize}
		
		\vspace{0.8em}
		\textbf{Learnable parameters}\\(for each edge)
		\begin{itemize}
			\item $t_i$: knot vectors, $i \in [1, m]$.
			\item $P_i$: control points (spline weights), $i \in [1, n]$.
			\item $w_b$: basis weight.
		\end{itemize}
		
	\end{columns}
	
\end{frame}

%------------------------------------------

\begin{frame}{KAN Backpropagation}
	Loss function is L2 (RMSE): 
	
	\begin{equation*}
		L = \left\| y - \hat{y} \right\|_2 = \left\| f(\mathbf{x}) - \hat{f}_{d}(\mathbf{x}) \right\|_2 = \left\| f(\mathbf{x}) - \sum_q \phi_{d,q}(\mathbf{x}) \right\|_2
	\end{equation*}
	
	Where $d$ is the last layer, and $p=1$ because we have a single output. The coefficients of that layer are $P_{d,q,i}$.
	
	\begin{equation*}
		\frac{\partial L}{\partial P_{d,q,i}} = \frac{\partial L}{\partial \hat{f}_{d}(\mathbf{x})} \cdot \frac{\partial \hat{f}_{d}(\mathbf{x})}{\partial P_{d,q,i}} 
	\end{equation*}
	
	And for the previous layer $d-1$:
	
	\begin{equation*}
		\frac{\partial L}{\partial P_{d-1,p,q,i}} = \frac{\partial L}{\partial \hat{f}_{d}(\mathbf{x})} \cdot \frac{\partial \hat{f}_{d}(\mathbf{x})}{\partial \hat{f}_{d-1,p}(\mathbf{x})} \cdot \frac{\partial \hat{f}_{d-1,p}(\mathbf{x})}{\partial P_{d-1,p,q,i}} 
	\end{equation*}
	
\end{frame}

%------------------------------------------

\begin{frame}{Capabilities of KANs with B-Splines}
	
	\begin{itemize}
		\item \textbf{Grid extension}: progressively increase model capacity by refining the spline grid without retraining from scratch.
		\item \textbf{Continual learning}: local support ensures new information affects only nearby regions, reducing catastrophic forgetting.
		\item \textbf{Sparsity}: regularization and pruning remove redundant components, simplifying the model without major accuracy loss.
		\item \textbf{Symbolic regression}: univariate structure enables conversion of learned functions into interpretable closed-form expressions.
	\end{itemize}
	
\end{frame}

%------------------------------------------

\begin{frame}{Capabilities - Grid Extension}
	\begin{columns}[T,onlytextwidth]
		
		% --- Left column: explanation ---
		\column{0.5\textwidth}
		\textbf{Grid extension} refines a trained KAN by adding more spline knots without restarting training.
		
		\begin{itemize}
			\item Train on a coarse grid first.
			\item Add knots to increase resolution and capacity.
			\item Initialize new coefficients by least-squares fitting.
			\item Continue training to improve accuracy.
		\end{itemize}
		
		Test loss often improves until the parameter count roughly matches the number of data points.
		
		% --- Right column: image ---
		\column{0.45\textwidth}
		\centering
		\includegraphics[height=0.8\textheight]{../images/grid_extension.png}
		
	\end{columns}
\end{frame}

%------------------------------------------

\begin{frame}{Capabilities - Continual Learning}
	
	Because B-splines have \textbf{local support}, updates to $\phi(x)$ in one region of the input space affect only nearby points.  
	This locality mitigates \textbf{catastrophic forgetting}, a common issue in MLPs where learning new data can overwrite previously acquired knowledge.
	
	\begin{figure}
		\centering
		\includegraphics[height=0.6\textheight]{../images/continual_learning.png}
	\end{figure}
	
\end{frame}


%------------------------------------------

\begin{frame}{Capabilities - Sparsity}
	\begin{columns}[T,onlytextwidth]
		
		% --- Left column: explanation ---
		\column{0.5\textwidth}
		
		\textbf{Sparsity} removes unnecessary components, revealing the essential structure of our target function.
		
		\vspace{0.6em}
		\begin{itemize}
			\item \textbf{Regularization} drives many spline weights toward zero.
			\item Irrelevant $\phi_{d,p,q}$ can be pruned after training.
			\item The result is a compact, interpretable network.
		\end{itemize}
		
		Sparsity helps towards \textbf{interpretability}.
		
		% --- Right column: diagram ---
		\column{0.45\textwidth}
		\centering
		\resizebox{!}{0.75\textheight}{\input{kan_2xy.tex}}
		
	\end{columns}
\end{frame}

%------------------------------------------

\begin{frame}{Capabilities - Symbolic Regression}
	
	KANs provide an interpretable path from neural models to closed-form expressions:
	
	\begin{itemize}
		\item Each learned $\phi_{d,p,q}$ is a univariate function, which can often be approximated by simple analytic forms (e.g., $\sin$, $\exp$, $\log$).
		\item After training, these functions are ``snapped'' to symbolic templates via affine fitting, producing human-readable equations.
		\item The resulting network can be viewed as a composition graph of symbolic functions approximating $f(x)$.
	\end{itemize}
	
	This makes KANs suitable not only for prediction but also for \textbf{discovering interpretable laws} from data.
	
\end{frame}

%------------------------------------------

\begin{frame}{KAN Train Steps}
	
	\begin{figure}
		\centering
		\includegraphics[height=0.8\textheight]{../images/kan_training.png}
	\end{figure}
	
\end{frame}

%------------------------------------------

\begin{frame}{Limitations of KANs with B-Splines}
	
	\begin{figure}
		\centering
		\includegraphics[height=0.8\textheight]{../images/kan_training.png}
	\end{figure}
	
\end{frame}

%------------------------------------------

\begin{frame}{References}
	\nocite{liu_kan_2025}
	\nocite{pal_understanding_nodate}
	\printbibliography[heading=none]
\end{frame}

\end{document}
