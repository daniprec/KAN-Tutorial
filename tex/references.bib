
@misc{liu_kan_2025,
	title = {{KAN}: {Kolmogorov}-{Arnold} {Networks}},
	shorttitle = {{KAN}},
	url = {http://arxiv.org/abs/2404.19756},
	doi = {10.48550/arXiv.2404.19756},
	abstract = {Inspired by the Kolmogorov-Arnold representation theorem, we propose KolmogorovArnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (“neurons”), KANs have learnable activation functions on edges (“weights”). KANs have no linear weights at all – every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability, on small-scale AI + Science tasks. For accuracy, smaller KANs can achieve comparable or better accuracy than larger MLPs in function fitting tasks. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful “collaborators” helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today’s deep learning models which rely heavily on MLPs.},
	language = {en},
	urldate = {2025-09-19},
	publisher = {arXiv},
	author = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Soljačić, Marin and Hou, Thomas Y. and Tegmark, Max},
	month = feb,
	year = {2025},
	note = {arXiv:2404.19756 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Statistics - Machine Learning},
	file = {PDF:C\:\\Users\\daniel.precioso\\Zotero\\storage\\FT5CGPHA\\Liu et al. - 2025 - KAN Kolmogorov-Arnold Networks.pdf:application/pdf},
}

@article{lei_tem2-kan_2025,
	title = {Tem2-{KAN}: {Data}-driven temporal temperature prediction via an improved {Kolmogorov}–{Arnold} network},
	issn = {00190578},
	shorttitle = {Tem2-{KAN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0019057825003635},
	doi = {10.1016/j.isatra.2025.07.014},
	language = {en},
	urldate = {2025-09-19},
	journal = {ISA Transactions},
	author = {Lei, Yongxiang and Deng, Bin and Wang, Ziyang},
	month = jul,
	year = {2025},
	pages = {S0019057825003635},
	file = {PDF:C\:\\Users\\daniel.precioso\\Zotero\\storage\\ELWKJJDI\\Lei et al. - 2025 - Tem2-KAN Data-driven temporal temperature prediction via an improved Kolmogorov–Arnold network.pdf:application/pdf},
}

@misc{akazan_localized_2025,
	title = {Localized {Weather} {Prediction} {Using} {Kolmogorov}-{Arnold} {Network}-{Based} {Models} and {Deep} {RNNs}},
	url = {http://arxiv.org/abs/2505.22686},
	doi = {10.48550/arXiv.2505.22686},
	abstract = {Weather forecasting is crucial for managing risks and economic planning, particularly in tropical Africa, where extreme events severely impact livelihoods. Yet, existing forecasting methods often struggle with the region’s complex, non-linear weather patterns. This study benchmarks deep recurrent neural networks such as LSTM, GRU, BiLSTM, BiGRU, and Kolmogorov-Arnold-based models (KAN and TKAN) for daily forecasting of temperature, precipitation, and pressure in two tropical cities: Abidjan, Cˆote d’Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce two customized variants of TKAN that replace its original SiLU activation function with GeLU and MiSH, respectively. Using station-level meteorological data spanning from 2010 to 2024, we evaluate all the models on standard regression metrics. KAN achieves temperature prediction (R2 = 0.9986 in Abidjan, 0.9998 in Kigali, MSE {\textless} 0.0014 ◦C2), while TKAN variants minimize absolute errors for precipitation forecasting in low-rainfall regimes. The customized TKAN models demonstrate improvements over the standard TKAN across both datasets. Classical RNNs remain highly competitive for atmospheric pressure (R2 ≈ 0.83−0.86), outperforming KAN-based models in this task. These results highlight the potential of spline-based neural architectures for efficient and data-efficient forecasting.},
	language = {en},
	urldate = {2025-09-19},
	publisher = {arXiv},
	author = {Akazan, Ange-Clement and Mbingui, Verlon Roel and N'guessan, Gnankan Landry Regis and Karambal, Issa},
	month = may,
	year = {2025},
	note = {arXiv:2505.22686 [cs]},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics},
	file = {PDF:C\:\\Users\\daniel.precioso\\Zotero\\storage\\MF69TEXV\\Akazan et al. - 2025 - Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs.pdf:application/pdf},
}

@article{alves_use_2024,
	title = {On the use of kolmogorov–arnold networks for adapting wind numerical weather forecasts with explainability and interpretability: application to madeira international airport},
	volume = {6},
	issn = {2515-7620},
	shorttitle = {On the use of kolmogorov–arnold networks for adapting wind numerical weather forecasts with explainability and interpretability},
	url = {https://iopscience.iop.org/article/10.1088/2515-7620/ad810f},
	doi = {10.1088/2515-7620/ad810f},
	abstract = {This study examines the application of machine learning to enhance wind nowcasting by using a Kolmogorov-Arnold Network model to improve predictions from the Global Forecast System at Madeira International Airport, a site affected by complex terrain. The research addresses the limitations of traditional numerical weather prediction models, which often fail to accurately forecast localized wind patterns. Using the Kolmogorov-Arnold Network model led to a substantial reduction in wind speed and direction forecast errors, with a performance that reached a 48.5\% improvement to the Global Forecast System 3 h nowcast, considering the mean squared error. A key outcome of this study comes from the model’s ability to generate mathematical formulas that provide insights into the physical and mathematical dynamics inﬂuencing local wind patterns and improve the transparency, explainability, and interpretability of the employed machine learning models for atmosphere modeling.},
	language = {en},
	number = {10},
	urldate = {2025-09-19},
	journal = {Environmental Research Communications},
	author = {Alves, Décio and Mendonça, Fábio and Mostafa, Sheikh Shanawaz and Morgado-Dias, Fernando},
	month = oct,
	year = {2024},
	pages = {105008},
	file = {PDF:C\:\\Users\\daniel.precioso\\Zotero\\storage\\IBJRZH2Y\\Alves et al. - 2024 - On the use of kolmogorov–arnold networks for adapting wind numerical weather forecasts with explaina.pdf:application/pdf},
}

@article{somvanshi_survey_2026,
	title = {A {Survey} on {Kolmogorov}-{Arnold} {Network}},
	volume = {58},
	issn = {0360-0300, 1557-7341},
	url = {http://arxiv.org/abs/2411.06078},
	doi = {10.1145/3743128},
	abstract = {This systematic review explores the theoretical foundations, evolution, applications, and future potential of Kolmogorov-Arnold Networks (KAN), a neural network model inspired by the Kolmogorov-Arnold representation theorem. KANs distinguish themselves from traditional neural networks by using learnable, spline-parameterized functions instead of fixed activation functions, allowing for flexible and interpretable representations of high-dimensional functions. This review details KAN's architectural strengths, including adaptive edge-based activation functions that improve parameter efficiency and scalability in applications such as time series forecasting, computational biomedicine, and graph learning. Key advancements, including Temporal-KAN, FastKAN, and Partial Differential Equation (PDE) KAN, illustrate KAN's growing applicability in dynamic environments, enhancing interpretability, computational efficiency, and adaptability for complex function approximation tasks. Additionally, this paper discusses KAN's integration with other architectures, such as convolutional, recurrent, and transformer-based models, showcasing its versatility in complementing established neural networks for tasks requiring hybrid approaches. Despite its strengths, KAN faces computational challenges in high-dimensional and noisy data settings, motivating ongoing research into optimization strategies, regularization techniques, and hybrid models. This paper highlights KAN's role in modern neural architectures and outlines future directions to improve its computational efficiency, interpretability, and scalability in data-intensive applications.},
	language = {en},
	number = {2},
	urldate = {2025-09-19},
	journal = {ACM Computing Surveys},
	author = {Somvanshi, Shriyank and Javed, Syed Aaqib and Islam, Md Monzurul and Pandit, Diwas and Das, Subasish},
	month = jan,
	year = {2026},
	note = {arXiv:2411.06078 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {1--35},
	file = {PDF:C\:\\Users\\daniel.precioso\\Zotero\\storage\\Z9JXURQ7\\Somvanshi et al. - 2026 - A Survey on Kolmogorov-Arnold Network.pdf:application/pdf},
}

@article{camps-valls_artificial_2025,
	title = {Artificial intelligence for modeling and understanding extreme weather and climate events},
	volume = {16},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-025-56573-8},
	doi = {10.1038/s41467-025-56573-8},
	language = {en},
	number = {1},
	urldate = {2025-09-19},
	journal = {Nature Communications},
	author = {Camps-Valls, Gustau and Fernández-Torres, Miguel-Ángel and Cohrs, Kai-Hendrik and Höhl, Adrian and Castelletti, Andrea and Pacal, Aytac and Robin, Claire and Martinuzzi, Francesco and Papoutsis, Ioannis and Prapas, Ioannis and Pérez-Aracil, Jorge and Weigel, Katja and Gonzalez-Calabuig, Maria and Reichstein, Markus and Rabel, Martin and Giuliani, Matteo and Mahecha, Miguel D. and Popescu, Oana-Iuliana and Pellicer-Valero, Oscar J. and Ouala, Said and Salcedo-Sanz, Sancho and Sippel, Sebastian and Kondylatos, Spyros and Happé, Tamara and Williams, Tristan},
	month = feb,
	year = {2025},
	pages = {1919},
	file = {PDF:C\:\\Users\\daniel.precioso\\Zotero\\storage\\Y29PL7XI\\Camps-Valls et al. - 2025 - Artificial intelligence for modeling and understanding extreme weather and climate events.pdf:application/pdf},
}

@article{polar_deep_2021,
	title = {A deep machine learning algorithm for construction of the {Kolmogorov}–{Arnold} representation},
	volume = {99},
	issn = {09521976},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197620303742},
	doi = {10.1016/j.engappai.2020.104137},
	abstract = {The Kolmogorov–Arnold representation is a proven adequate replacement of a continuous multivariate function by a hierarchical structure of multiple functions of one variable. The proven existence of such representation inspired many researchers to search for a practical way of its construction, since such model answers the needs of machine learning. This article shows that the Kolmogorov–Arnold representation is not only a composition of functions but also a particular case of a tree of the discrete Urysohn operators. The article introduces new, quick and computationally stable algorithm for constructing of such Urysohn trees. Besides continuous multivariate functions, the suggested algorithm covers the cases with quantised inputs and combination of quantised and continuous inputs. The article also contains multiple results of testing of the suggested algorithm on publicly available datasets, used also by other researchers for benchmarking.},
	language = {en},
	urldate = {2025-10-02},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Polar, A. and Poluektov, M.},
	month = mar,
	year = {2021},
	pages = {104137},
	file = {PDF:C\:\\Users\\daniel.precioso\\Zotero\\storage\\4YRL3LJ7\\Polar and Poluektov - 2021 - A deep machine learning algorithm for construction of the Kolmogorov–Arnold representation.pdf:application/pdf},
}

@article{igelnik_kolmogorovs_2003,
	title = {Kolmogorov's spline network},
	volume = {14},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1045-9227},
	url = {http://ieeexplore.ieee.org/document/1215392/},
	doi = {10.1109/TNN.2003.813830},
	abstract = {In this paper, an innovative neural-network architecture is proposed and elucidated. This architecture, based on the Kolmogorov’s superposition theorem and called the Kolmogorov’s spline network (KSN), utilizes more degrees of adaptation to data than currently used neural-network architectures (NNAs). By using cubic spline technique of approximation, both for activation and internal functions, more efficient approximation of multivariate functions can be achieved. The bound on approximation error and number of adjustable parameters, derived in this paper, favorably compares KSN with other one-hidden layer feedforward NNAs. The training of KSN, using the ensemble approach and the ensemble multinet, is described. A new explicit algorithm for constructing cubic splines is presented.},
	language = {en},
	number = {4},
	urldate = {2025-10-02},
	journal = {IEEE Transactions on Neural Networks},
	author = {Igelnik, B. and Parikh, N.},
	month = jul,
	year = {2003},
	pages = {725--733},
	file = {PDF:C\:\\Users\\daniel.precioso\\Zotero\\storage\\B798BA6C\\Igelnik and Parikh - 2003 - Kolmogorov's spline network.pdf:application/pdf},
}

@article{pal_understanding_nodate,
	title = {Understanding the {Limitations} of {B}-{Spline} {KANs}: {Convergence} {Dynamics} and {Computational} {Efficiency}},
	abstract = {Kolmogorov-Arnold Networks (KANs) have recently emerged as a potential alternative to multi-layer perceptrons (MLPs), leveraging the Kolmogorov Representation Theorem to introduce learnable activation functions on each edge rather than fixed activations at the nodes. While KANs have demonstrated promise in small-scale problems by achieving similar or better performance with fewer parameters, our empirical investigations reveal significant limitations when scaling to real-world tasks. Specifically, KANs suffer from increased computational costs and reduced performance, rendering them unsuitable for deep learning applications. Our study explores these limitations through extensive testing across diverse tasks, including computer vision and scientific machine learning, and provides a detailed comparison with MLPs.},
	language = {en},
	author = {Pal, Avik and Das, Dipankar},
	file = {PDF:C\:\\Users\\daniel.precioso\\Zotero\\storage\\ILAPGGCT\\Pal and Das - Understanding the Limitations of B-Spline KANs Convergence Dynamics and Computational Efficiency.pdf:application/pdf},
}
